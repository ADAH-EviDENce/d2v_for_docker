{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model generation using doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering a research question based on information within a corpus of text relies on the ability to extract relevant subsets of the corpus (documents, parts of documents, etc.). The defintion of what constitutes a relevant subset may, however, not be immediately implementable in classical information retrieval terms such as keywords (which may also be perceived as limiting), and is not an intuitive starting point for researchers accustomed to working close to text, i.e. close-readng documents to identify and retrieve the information which interests them.\n",
    "\n",
    "As an alternative to classical keyword defintion, and inspired by the approach of identifying relevant documents or passages using a more holistic assesment of their content, in the following we present the construction of a shallow neural network based model, trained on a user supplied corpus (which has been preprocessed by the user), aiming to encapsualte the content of a corpus element, with the goal, of enabling the user to retrieve elements of similar content by querying the corpus using the content of a seed element as numerically encoded by the model.\n",
    "\n",
    "In the fllowing we construct the model using the `gensim` Python package and the doc2vec model framework it provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The following steps set up our environment\n",
    "\n",
    "First import the standard and required framework packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next import packages directly related to the model construction and serialization of model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from filepaths.ipynb\n",
      "importing Jupyter notebook from corpus_reinferral.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import filepaths as fp\n",
    "import corpus_reinferral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as model construction is a process which requires a significant amout of computation, import packages to enable efficent usage of the available computational infrastructure, and specify allowed usage of infrastructure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import dask\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "usecores=np.int(3*cores/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get full file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file_path = fp.corpus_file_path\n",
    "corpus_ids_file_path = fp.corpus_ids_file_path\n",
    "model_output_file_path = fp.model_output_file_path\n",
    "model_output_corpus_vectors_path = fp.model_output_corpus_vectors_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having setup the environment we next define a small number of utility functions which will enable us to import the (already preprocessed) corpus and subsequently train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to read in corpus into input format supported by `gensim` `doc2vec`\n",
    "\n",
    "This assumes that the corpus token file has been produced following the process encoded in the pre-processing notebook (), i.e. each corpus element (a 'document' in the sense of the doc2vec model) to be included in the model has been preprocessed and tokenized and is stored as a single string of tokens, with one element ('document') per line.\n",
    "\n",
    "We note, that in many cases these elements/documents will, in fact, correspond to e.g. paragraphs of a larger document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_file):\n",
    "    with smart_open(corpus_file,'r') as tf:\n",
    "        for i,text_line in enumerate(tf):\n",
    "            tokens = text_line.split(' ')\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens,[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to read in corpus and list of corpus element file names, creating an object which supports inspecting and linking model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_lookup(corpus_ids_file, corpus_file) :\n",
    "    with smart_open(corpus_ids_file, 'r') as fnf, smart_open(corpus_file,'r') as tf:\n",
    "        i=0\n",
    "        for (fn_line,tf_line) in zip(fnf,tf):\n",
    "            yield ([i],[fn_line.rstrip()],[tf_line])\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the utility functions defined we can now load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = list(read_corpus(corpus_file_path))\n",
    "#import pickle\n",
    "#with open('/Users/eslt0101/Data/eScience/Evidence/Data/NR-Teksts/EviDENce_NR_output_clean/TargetSize150/corp_clean.pkl','rb') as f:\n",
    "#    corp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the lookup corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_lookup = list(read_corpus_lookup(corpus_ids_file_path,corpus_file_path))\n",
    "#with open('/Users/eslt0101/Data/eScience/Evidence/Data/NR-Teksts/EviDENce_NR_output_clean/TargetSize150/corp_lookup_clean.pkl','rb') as f:\n",
    "#    corp_lookup = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique identifiers for each corpus element can then be constructed from the lookup corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This assumes that all corpus file names will end with .txt\n",
    "corp_ids =[]\n",
    "for i in range(len(corp)):\n",
    "    corp_ids.append(corp_lookup[i][1][0].split('.txt')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having imported the corpus we can now build a model from it.\n",
    "\n",
    "First we specify the model we want to bulid. In this case that is doc2vec with largely default settings, with modifications as specified in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension=50\n",
    "word_min_count=2\n",
    "number_of_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_dimension, min_count=word_min_count, epochs=number_of_epochs, workers=usecores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, using the model, we build the vocabulary that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we train the model, timing the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.23 s, sys: 1.3 s, total: 5.53 s\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(corp, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the resulting trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_output_file_path)\n",
    "#model = gensim.utils.SaveLoad.load('/Users/eslt0101/Projects/EviDENce/ML/model_default_v50_mc2_e30_freeze_clean.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinfer corpus vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant sections of the corpus vectors have been constructed during early epochs of the model training. Furthermore, in general, the corpora being modelled will be relatively small, so that individual instances of a derived/infered vector may be unstable and fluctuate in component dimensions. To adress this issue we reinfer the vector for each element of the corpus using the fully trained and frozen model multiple times, using the component wise median as the descriptive vector associated with a corpus element in further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use imported reinferral engine (corpus_reinferral.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinferral settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_reinferral_instances=100  #this corresponds to the default setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute reinferral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinferred_corpus_vectors = corpus_reinferral.reinfer_corpus_single(corp,model,reinferral_instances=number_reinferral_instances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having reinferred the corpus vectors the results are saved in `numpy` binary format for potenial later (rapid) use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reinferred vectors save above require a separate mapping structure to ensure the correct association of a vector with the corresponding corpus element in a more general use scenario which might include reordering. To this end, we save a dataframe containing the reinferred vectors and the associated element names/ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(zip(corp_ids,reinferred_corpus_vectors))\n",
    "vectorDF = pd.DataFrame(data_list,columns=['id','vector'])\n",
    "vectorDF.to_pickle(model_output_corpus_vectors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code above, we have built a doc2vec model on the ALREADY PREPROCCESSED corpus supplied, and have constructed averaged (i.e. numerically stabablized) vectors in the model space for all corpus elements. Both the model and the vectors have been serialized for further use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EviDENce]",
   "language": "python",
   "name": "conda-env-EviDENce-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
